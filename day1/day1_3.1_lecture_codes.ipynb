{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">레이어</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "fc_layer = nn.Linear(7*7*32, 1)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "sequential_layer1 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fc_layer)\n",
    "print(conv_layer)\n",
    "print(sequential_layer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">모델</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BasicNet(nn.Module):               \n",
    "  def __init__(self, output_shape=10):        \n",
    "    super(BasicNet, self).__init__()        \n",
    "    self.fc = nn.Linear(32*32*3, output_shape)   \n",
    "  def forward(self, x):                \n",
    "    out = self.fc(x)                \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCNet(nn.Module):                  \n",
    "  def __init__(self, num_classes=10):         \n",
    "    super(FCNet, self).__init__()          \n",
    "    self.fc0 = nn.Linear(32*32*3, 256)       \n",
    "    self.fc1 = nn.Linear(256, 128)         \n",
    "    self.fc2 = nn.Linear(128, 64)          \n",
    "    self.fc3 = nn.Linear(64, 32)          \n",
    "    self.fc4 = nn.Linear(32, num_classes)\n",
    "        \n",
    "def forward(self, x):                 \n",
    "    out0 = self.fc0(x)\n",
    "    out1 = self.fc1(out0)\n",
    "    out2 = self.fc2(out1)\n",
    "    out3 = self.fc3(out2)\n",
    "    out4 = self.fc4(out3)\n",
    "    score = self.fc(out4) # Score\n",
    "    # score = F.softmax(score)\n",
    "    return score \n",
    "  \n",
    "model = FCNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Linear model (width*height*channel of the last feature map, Number of class)\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        flatten = out.view(out.size(0), -1)  # Flatten\n",
    "        score = self.fc(flatten) # Score\n",
    "        # score = F.softmax(score) \n",
    "        return score\n",
    "    \n",
    "model = ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.models.detection as detection\n",
    "\n",
    "all_models = models.list_models()\n",
    "classification_models = models.list_models(module=models)\n",
    "detection_models = models.list_models(module=detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detection_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined Model\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "model = models.resnet18(weights=None)\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">손실 함수</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "prediction = torch.tensor([1.2, 0.4, 0.2, 0.2])\n",
    "groundTruth = torch.tensor([1.0, 0.0, 0.0, 0.0])\n",
    "loss = criterion(prediction, groundTruth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">옵티마이저</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(2,2, bias=False)\n",
    "torch.nn.init.ones_(model.weight.data)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# output1\n",
    "optimizer.zero_grad()\n",
    "input = torch.tensor([1.0,4.0])\n",
    "output = model(input)\n",
    "loss1 = criterion(input, output)\n",
    "loss1.backward()\n",
    "optimizer.step()\n",
    "print(output.data)\n",
    "print(loss1.data)\n",
    "print(model.weight.data)\n",
    "\n",
    "#output2\n",
    "optimizer.zero_grad()\n",
    "output = model(input)\n",
    "loss2 = criterion(input, output)\n",
    "loss2.backward()\n",
    "optimizer.step()\n",
    "print(output.data)\n",
    "print(loss2.data)\n",
    "print(model.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">데이터셋</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13269,
     "status": "ok",
     "timestamp": 1684644697312,
     "user": {
      "displayName": "허윤재",
      "userId": "12083611387958911250"
     },
     "user_tz": -540
    },
    "id": "_NSB6AqzpCAA",
    "outputId": "05bfe48a-a90f-4f7f-eada-932965d24f08"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import os\n",
    "data_root = os.path.join(os.path.dirname(os.getcwd()), 'data', 'cifar10')\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=data_root, # 불러올 데이터가 있는 path\n",
    "    train=True, # train data를 불러올거면 True, test data를 불러올거면 False\n",
    "    download=True, # 데이터가 존재하지 않으면 root에 해당 데이터셋을 다운로드\n",
    "    transform=ToTensor() # dataset에 적용할 transform 지정\n",
    ")\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=data_root,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1684644710514,
     "user": {
      "displayName": "허윤재",
      "userId": "12083611387958911250"
     },
     "user_tz": -540
    },
    "id": "gYm_r2_Wpgth",
    "outputId": "b37388ed-24a3-417e-9e70-a6ebe3d36938"
   },
   "outputs": [],
   "source": [
    "# dimension order in torch : (channel, height, width)\n",
    "print(train_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 1663,
     "status": "ok",
     "timestamp": 1684645000518,
     "user": {
      "displayName": "허윤재",
      "userId": "12083611387958911250"
     },
     "user_tz": -540
    },
    "id": "eXZukHJuqFNJ",
    "outputId": "4b53a885-d68d-4e9a-906e-bfd5590b539a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = train_data[0][0]\n",
    "plt.imshow(image.permute(1,2,0)) # order in plt should be (height, width, channel)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import PIL.Image as Image\n",
    "import pickle\n",
    "\n",
    "class CIFAR10(Dataset):\n",
    "    base_folder = \"cifar-10-batches-py\"\n",
    "    train_list = [\n",
    "        [\"data_batch_1\", \"c99cafc152244af753f735de768cd75f\"],\n",
    "        [\"data_batch_2\", \"d4bba439e000b95fd0a9bffe97cbabec\"],\n",
    "        [\"data_batch_3\", \"54ebc095f3ab1f0389bbae665268c751\"],\n",
    "        [\"data_batch_4\", \"634d18415352ddfa80567beed471001a\"],\n",
    "        [\"data_batch_5\", \"482c414d41f54cd18b22e5b47cb7c3cb\"],\n",
    "    ]\n",
    "    test_list = [\n",
    "        [\"test_batch\", \"40351d587109b95175f43aff81a1287e\"],\n",
    "    ]\n",
    "    meta = {\n",
    "        \"filename\": \"batches.meta\",\n",
    "        \"key\": \"label_names\",\n",
    "        \"md5\": \"5ff9c542aee3614f3951f8cda6e48888\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None,target_transform=None):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        if self.train:\n",
    "            downloaded_list = self.train_list\n",
    "        else:\n",
    "            downloaded_list = self.test_list\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        for file_name, _ in downloaded_list:\n",
    "            file_path = os.path.join(self.root, self.base_folder, file_name)\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                entry = pickle.load(f, encoding=\"latin1\")\n",
    "                self.data.append(entry[\"data\"])\n",
    "                if \"labels\" in entry:\n",
    "                    self.targets.extend(entry[\"labels\"])\n",
    "                else:\n",
    "                    self.targets.extend(entry[\"fine_labels\"])\n",
    "\n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))\n",
    "        self._load_meta()\n",
    "\n",
    "    def _load_meta(self):\n",
    "        path = os.path.join(self.root, self.base_folder, self.meta[\"filename\"])\n",
    "        with open(path, \"rb\") as infile:\n",
    "            data = pickle.load(infile, encoding=\"latin1\")\n",
    "            self.classes = data[self.meta[\"key\"]]\n",
    "        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_root = os.path.join(os.path.dirname(os.getcwd()), 'data', 'cifar10')\n",
    "\n",
    "train_data = CIFAR10(\n",
    "    root= data_root,\n",
    "    train=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "print(len(train_data))\n",
    "\n",
    "image = train_data[3][0]\n",
    "plt.imshow(image.permute(1,2,0)) # order in plt should be (height, width, channel)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">어그멘테이션</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1684645200361,
     "user": {
      "displayName": "허윤재",
      "userId": "12083611387958911250"
     },
     "user_tz": -540
    },
    "id": "oVqqmGFEqszh",
    "outputId": "036a7d93-4591-433d-8d85-159ed74ca5d1"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Augmentation들 선언\n",
    "normalize =transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "center_crop = transforms.CenterCrop(16)\n",
    "horizontal_flip = transforms.RandomHorizontalFlip(p=1)\n",
    "vertical_flip = transforms.RandomVerticalFlip(p=1)\n",
    "random_roation = transforms.RandomRotation([-45, 45])\n",
    "gray_scale = transforms.Grayscale()\n",
    "\n",
    "#이미지에 각각의 Augmentation을 적용\n",
    "normalized_img = normalize(image)\n",
    "center_crop_img = center_crop(image)\n",
    "h_flip_img = horizontal_flip(image)\n",
    "v_flip_img = vertical_flip(image)\n",
    "rotate_img = random_roation(image)\n",
    "gray_img = gray_scale(image)\n",
    "\n",
    "# 적용된 Augmentation 결과들을 확인\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "for i in range(1,12,2):\n",
    "    plt.subplot(6,2,i)\n",
    "    plt.imshow(image.permute(1,2,0))\n",
    "    plt.title('original')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,2)\n",
    "plt.imshow(normalized_img.permute(1, 2, 0))\n",
    "plt.title('normalize')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,4)\n",
    "plt.imshow(center_crop_img.permute(1,2,0))\n",
    "plt.title('center crop')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,6)\n",
    "plt.imshow(h_flip_img.permute(1, 2, 0))\n",
    "plt.title('horizontal flip')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,8)\n",
    "plt.imshow(v_flip_img.permute(1, 2, 0))\n",
    "plt.title('vertical flip')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,10)\n",
    "plt.imshow(rotate_img.permute(1, 2, 0))\n",
    "plt.title('rotation')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(6,2,12)\n",
    "plt.imshow(gray_img.permute(1, 2, 0), cmap='gray')\n",
    "plt.title('gray scale')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:150%\">데이터로더</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1684644754832,
     "user": {
      "displayName": "허윤재",
      "userId": "12083611387958911250"
     },
     "user_tz": -540
    },
    "id": "fCJtS5pVpk9I",
    "outputId": "cc084632-81d9-45ba-954a-33f4e25b0fec"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=256, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(test_data, batch_size=256, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))\n",
    "print(data[0].shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODUwU+N1PKdelHWfSPv/bJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
